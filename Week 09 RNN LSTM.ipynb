{"cells":[{"cell_type":"markdown","metadata":{"id":"eDbvjh4I5zKH"},"source":["# Recurrent Neural Networks and LSTM\n","\n","## Introduction\n","\n","Recurrent Neural Networks are one of the most popular Neural Network architecture types due to their importance in processing sequential and time series data. While time-series data usually refers to real valued data with a temporal dimension such as stock market prices and sensor outputs, sequential data is a much broader data type which also includes audio, text, and video.\n","\n","We begin with an overview of basic RNNs with a simple example. After that we look at LSTMs as perhaps the most popular example of a traditional recurrent neural network. Later in the lecture we start looking at the topic of generative machine learning by seeing how recurrent neural networks can be used to create a type of langauge model.\n","\n","Note that some of the notes below are based on the tutorial here:\n","https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n","\n","Transformer Architectures and the idea of attention are also really important in processing sequential data. We won't cover them this week, but we will have a look at these in detail in the next lecture."]},{"cell_type":"markdown","metadata":{"id":"0Y-3Ugl75zKS"},"source":[" ## Recurrent Neural Networks - Overview\n","\n","A recurrent neural network is a neural network in which the output of the hidden state of input $i$ is fed to the hidden state alongside the $i+1^{th}$ input. That way, the output at time step k is not only dependent on the input at time step k, but also on the inputs up to point k.\n","\n","We can illustrate the most basic RNN instance below with a hidden neuron which is connected back to itself.\n","\n","<!-- rnn.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1dZpj6awgBNmpLZSnCIYxU5kciLcp-nP0\"/>\n","\n","We can 'unfold' such a design in time to get a better sense of what is happening. In the figure below we see the RNN with three inputs applied in sequence. The input t=0 will result in a hidden state which is passed to the output at time t=0 but which is also passed into the hidden state at t=1. Thus at t=1 the output of the hidden neuron is dependent not only on the input, but also on the hidden state variable from time t=0. Similarly at t=2 the output of the hidden neuron is dependent on the input at t=2 but also on the output of the hidden state from t=1 which in turn was dependent on the output of the hidden state at t=0. This shows the power of the model, i.e., the hidden state output at t=2 is not only dependent on the input variable at t=2, but on all the inputs that have been seen to date.\n","\n","<!-- rnn2.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1OeVajqjv27Z44wV3e6immRGxeK-jp6wb\"/>\n","\n","The example above has just one input and output neuron, thus we only have one target variable and one feature. We can trivially extend this approach to multiple target variables and input features by connecting each hidden neuron to its corresponding neuron in subsequent instances of the network as it is unfolded over time. This is illustrated below. Note that for visual purposes we mark hidden-to-hidden links with a separate color, but that this has no bearing on the implementation.\n","\n","<!-- rnn5.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1gljfKYL5kduubxU5rv_w_qzT9CNf8qrK\"/>\n","\n","This is a vanilla form of recurrent neural network. In practice the architecture adopted is much more complex than this and allows us a greater degree of control in determining what parts of the state variable is saved or passed on to future time steps. But for the moment let's see how this type of model can be put to work."]},{"cell_type":"markdown","metadata":{"id":"OePxVkoc5zKT"},"source":["## Recurrent Neural Networks - Model Description\n","\n","For our Vanilla RNN, let us consider the most basic form of RNN possible, i.e., a RNN which has a link to a single neuron in an input layer, a single neuron in the output layer, and a state variable that is one neuron wide. Lets also assume that the input to this network, X, is a stream of values $\\in R$ and that the output from the network, Y, is a stream of values $\\in \\{0,1\\}$. We will refer to the output of the hidden RNN unit as its state, S, and this state is passed to the logit calculations of the output layer and also to the logit calculations of the hidden layer itself. The architecture for this network is illustrated below:\n","\n","<!-- rnn_vanilla.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1Nej2Ng92K-hjWK0Uz64GIMtIgrdXf0ce\"/>\n","\n","Some points to note:\n"," * Our input layer is clamped to input values $x \\in X$ in the normal way.\n"," * Our output layer is in this case a hyperbolic tangent function (softmax is not necessary as we only have one output binary class).\n"," * Our hidden unit is recursive in that its output value is fed back and concatenated with the content from the input layer.\n"," * Weights and biases are assumed for the hidden and output layers in the normal way. We will return to the dimensionality of these variables later.\n","\n","Operationally the hidden or recursive unit behaves like a typical hidden layer in that a logit is first computed and from this an activation is calculated. We will assume that the activation value is a hyperbolic tangent function, but this need not be the case. The activation function for our vanilla RNN could be a logistic function or other suitable activation function.\n","\n","Our visualization above simplifies the temporal nature of the RNN as it shows the output of the RNN unit being fed directly back in to the same unit. For clarity it is useful to think about the network as having to be unrolled out over time to deal with sequences of inputs. We visualizae this below by extending the network out to a sequence of 3 input units at T=0, T=1, and T=2.\n","\n","<!-- rnn_vanilla_unrolled.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1my7wmbnwIs_AT7h5DOX7Vs-NMIS87GUS\"/>\n","\n","Referring to the above we can see now how the output of the hidden layer at time T is passed as input to the hidden layer at T+1. Specifically the $S_{T}$ is concatenated with the actual input $X_{T+1}$. At time T=1 we don't have a true hidden state from a previous step to concatenate with our input $X_{0}$. In this case we typically use an initial value for S which we set to 0.\n","\n","As noted above, the output of the hidden RNN unit is commonly referred to as the hidden state, $S$. The value of S for a given time point $t$ is simply:\n","\n","$$S_{t} = tanh( (W * (X \\oplus S_{t-1})) + b) $$\n","\n","where $\\oplus$ denotes vector concatenation. Thus we see that for a given point in time that the calculation of the hidden layer is typical of any feedforward layer that we have dealt with to date. The difference is that this value is passed back to the network at time $t+1$ as well as being passed to the logit calculation for the output layer.\n","\n","### Hidden State Size\n","In our figures above we assumed our hidden layer had just one hyperbolic tangent unit. This corresponds to having a hidden state size of 1. Naturally however having only one channel to carry information across time will limit the complexity of temporal features that our model will be able to learn. Therefore, in practice we will want a hidden state that is much wider.\n","\n","Our equation for calculating $S$ above has no such assumptions about the size of the hidden state, and indeed we will see later our implementations of RNNs are built to accommodate much larger hidden states. The approach taken to this is very simple. Rather than having a single non-linear unit, we make use of a so-called RNN cell which can have an arbitrary state size. Internally the cell is implemented as a collection of neurons, i.e., logit and non-linearity calculations, but the specifics of these internal operations are abstracted for us.\n","\n","We can illustrate this approach by looking again at the case where T=1 and assume instead a hidden state size of 4. In this illustration $S_{0}$ is the output state from the $0^{th}$ application of the model, and $S_{1}$ will be fed to the following application.\n","\n","<!-- rnn_vanilla_states.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1ZQlHUkDtnhdBp0FdQBK3zefPOebzAZbz\"/>\n","\n","By convention we won't illustrate each of the neurons within the RNN cell and will instead only illustrate the RNN cell itself. For our purposes here we will use a square to capture such RNN cells as opposed to the ovals we have used to date. As above we split the cell in half to illustrate the point that a logit is first calculated before a non-linearity.\n","\n","<!-- rnn_vanilla_unrolled_2.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1BmlEzu11ckYmMBws3uE6vFzflbz9RgiT\"/>\n","\n","### Aside: Unrolling the Vanilla RNN for Training\n","\n","Above we unrolled the Vanilla RNN over a number of time steps in order to make its operation more transparent. It turns out however that unrolling the RNN is in fact essential to its use.\n","\n","Lets consider first the case of using our Vanilla RNN model where the model parameters (i.e., the weights and bias for the hidden and output layers) have already been set. In this case it should be possible for us to feed an input sequence one unit at a time through an unrolled network and produce a set of valid outputs. We have to be careful to copy our output from time point $t$ to time point $t+1$ for concatenation with our actual input, but otherwise there should not be a problem.\n","\n","However for training, things are a little bit more complex. At the end of an input sequence we should be able to calculate a total training loss on the sequence. That loss is not just from the error on the final output symbol, but should instead be based on the true versus predicted value for the output Y at each time step. We need to propagate errors back from the last time step through multiple steps.\n","\n","One question is how many steps should we propagate errors backwards for? There are two extreme conditions. In the first extreme condition we don't propagate errors backwards in time at all. In this case we will not be able to learn temporal dependencies in our data - which defeats the whole purpose of what we are trying to achieve. In the other extreme we allow errors on the final symbol to propagate all the way back, so that in theory corrections to weights at the first time step could be influenced by an error in the final symbol. While this might be appealing in that it would allow arbitrarily long dependencies to be identified, in practice it does not work for two reasons. The first is a purely resource based concern. The further back we allow error propagation, the more complex our model and the more resources (time and memory) that are consumed. The second issue however is more problematic and is an instantiation of the vanishing gradient problem. Essentially over a long time step the repeated multiplication of error gradients will result in the gradient approaching either 0 or $\\infty$.\n","\n","The solution then in practice is to limit the propagation of errors to a fixed number of time steps. This is referred to as truncated backpropagation.\n","\n","### Multiple RNN Layers\n","\n","In the example above we used a single layer of LSTM units. In practice though we can use multiple layers where the output of one layer feeds into the next layer. This can be visualized as follows:\n","\n","<!-- rnn_multi_layer.png -->\n","<img width=\"600\" src=\"https://drive.google.com/uc?id=12CQWepvinVskswapE_GohcZ8apEwslp4\"/>\n","\n","When we are dealing with the output of an RNN, please keep in mind however that it can be used in two different ways. In one circumstance we may only be interested in the final output of the RNN, i.e., what was the output produced for the final chunk of input. This might be relevant if we want a simple classifier based almost entirely on the RNN's own internal processing. In another circumstance we might want to see the output of the RNN for each step. Here we have an output for each individual time step. This is often useful to further classifier steps as they can make a decision not just based on the final output of the RNN, but also on the intermediary decisions that were made by it. Typical implementations, like for example provided by Tensorflow, allow you to parameterise whether an RNN cell outputs only its final result, or its result unfolded in time."]},{"cell_type":"markdown","metadata":{"id":"pgtI32b_5zKX"},"source":["## Usage Scenario 1: Alarm Tripping\n","\n","To illustrate the model above, we will create a Trip Alarm for Negative Inputs. This alarm will produce as output the value 0 so long as the input is positive. However if ever the input value turns negative the output of the alarm will switch to output 1 and remain at 1 regardless of the inputs.\n","\n","### Step 1 - Data Creation\n","We can easily generate some sample data that corresponds to our usage scenario. For training we will limit all our examples to being constant length (currently 20 units below in the code). We will also set the input data to be in the range -1 to 20. We also generate the Target data straightforwardly by iterating over our dataset and applying a function that checks for the occurrence of negative values, and if found sets the output to 1 from the point at which the negative value is found, until the end of the sequence. We will create a large training data set and a smaller test data set with the same methods."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8163,"status":"ok","timestamp":1710855503946,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"UKJ74smi5zKa","outputId":"58b49317-2c25-46a8-8a17-a7bde1b0c705"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 4. 18. 16.  5. 16. 17. 13.  4. 17.  7.  9. 17. 17. 13. 19.  8.  4.  0.\n","   6.  8.]]\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# set this below to \"./\" if you just want to use the current directory to save temporary model files\n","# else set it to whereever makes sense for you\n","my_temp_folder = \"./\"\n","\n","# Set up some variables used in data collection and in training\n","num_training_examples = 10000\n","num_test_examples = 100\n","example_length = 20\n","lower_bound = -1\n","upper_bound = 20\n","\n","# Create Training and Test input variables\n","X_train = np.random.randint(low=lower_bound,high=upper_bound,\n","                      size=(num_training_examples,example_length)).astype(float)\n","X_test = np.random.randint(low=lower_bound,high=upper_bound,\n","                      size=(num_test_examples,example_length)).astype(float)\n","\n","# Create Training and Test target variable\n","def scan_example(x):\n","    \"\"\"Function Used to Create Individual Target Vectors by Scanning Input Vectors\"\"\"\n","    y = np.zeros(shape=np.shape(x))\n","    for index,val in np.ndenumerate(x):\n","        if val < 0:\n","            y[index[0]:] = 1\n","            break\n","    return y\n","Y_train = np.apply_along_axis(scan_example,1,X_train)\n","Y_test = np.apply_along_axis(scan_example,1,X_test)\n","\n","# For processing with Tensorflor / Keras, we expand the dimensionality of our data by one.\n","# In other words our inputs will be sets of sets of individual values -- rather than sets of arrays\n","X_train = np.expand_dims(X_train, axis=2)\n","X_test = np.expand_dims(X_test, axis=2)\n","Y_train = np.expand_dims(Y_train, axis=2)\n","Y_test = np.expand_dims(Y_test, axis=2)\n","\n","# Print examples from the test data (we add the T to transpose -- this just means the printing is neater. Remove it if you want to see what happens withut it. )\n","print(X_test[0].T)\n","print(Y_test[0].T)"]},{"cell_type":"markdown","metadata":{"id":"epHeBMOj5zKc"},"source":["### Step 2: Model Creation\n","\n","We will build our recurrent model for this 'alarm system' using a Keras wrapper for Tensorflow."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710855503946,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"-Tgo6-135zKc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"lfbnNEmE5zKd"},"source":["Next, lets lust define our batch size etc. We will use a state_size of 8. This is already pretty small, but you can still get results if you reduce it to 4. It just might take longer to train."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710855503947,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"NyheAmUS5zKd"},"outputs":[],"source":["batch_size = 100\n","state_size = 8\n","num_classes = 1"]},{"cell_type":"markdown","metadata":{"id":"MyBzPpRY5zKd"},"source":["Next, let's define our model. We will be able to build our model based around the 'Sequential' style constructor that Keras uses. Keep in mind that not ever model that people build is sequential. For now though, it is perfect."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1710855505496,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"WsXKs1n45zKe"},"outputs":[],"source":["model = tf.keras.Sequential()"]},{"cell_type":"markdown","metadata":{"id":"JfM0sXO45zKe"},"source":["Next, let's define our RNN layer. We need to specify the hidden state size. We also ask Tensorflow not just to output values for every step in the sequence -- not just the final step. Finally, to help guide tensorflow we give it some guidence on the input data dimensions it should expect; here 20 is the length of each example and 1 is the dimensionality of each input element, i.e., just a single number. We don't need to specify the batch size, Keras assumes we will have data in batches -- even if they are just batches of length 1."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1260,"status":"ok","timestamp":1710855506753,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"25_6BBQb5b6M"},"outputs":[],"source":["# Input will be passed to an basic RNN layer - sequences will be generated\n","model.add(layers.SimpleRNN(state_size,return_sequences=True,input_shape=(20,1)))"]},{"cell_type":"markdown","metadata":{"id":"SIBzzumR5zKe"},"source":["Next, we are going to let the full output of the recurrent network be passed to a single output unit. The output unit will be logistic since we are dealing with a single simple classification tasks."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1710855506754,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"gZ5T0jSP5zKf"},"outputs":[],"source":["# add an output layer consisting of 1 single logistic classifier\n","model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid))"]},{"cell_type":"markdown","metadata":{"id":"AuqlOZnH5zKf"},"source":["Next, we need to compile our model. We use cross entropy since we want our targets are 0s and 1s and our final layer neuron is pretty good at predicting them. We will use the adam optimiser, and ask for an accuracy metric to be calculated."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1710855506754,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"L4k-9zCB5zKf"},"outputs":[],"source":["model.compile(loss=\"binary_crossentropy\",\n","                   optimizer='adam',\n","                   metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"6KSHx7jK5zKf"},"source":["Let's have a look at the built, but not yet trained model."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1710855506981,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"yzzd84QV5zKg","outputId":"ba9f032b-2281-44f7-9ee8-a2c7d655a631"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," simple_rnn (SimpleRNN)      (None, 20, 8)             80        \n","                                                                 \n"," dense (Dense)               (None, 20, 1)             9         \n","                                                                 \n","=================================================================\n","Total params: 89 (356.00 Byte)\n","Trainable params: 89 (356.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"tvFJY1_yAGSf"},"source":["Note that our output is a vector of 20 elements, i.e., it is a sequence. Our loss function will be calculating a total loss across all 20 elements. Loss will be calculated on an item by item basis. If we have made the correct prediction at some point, we do not portion some of the loss / blame to the unit at that point, i.e., we don't average the losses back out over the 20 elements."]},{"cell_type":"markdown","metadata":{"id":"EY-CMWis5zKh"},"source":["We can see that in the great scheme of things this is a very small model. We could in practice make it much smaller by reducing the size of the hidden state. If we did this our model would likely still converge to a good model, but it would not do so as frequently. We might have to rerun training to get a good model.  \n","\n","Next, time to train the model. We provide input and output data, and specify a batch size for training as well as the number of epochs. If the model didn't train for you, increase the number of epochs."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":145905,"status":"ok","timestamp":1710855652886,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"sYvcAE7y5zKh"},"outputs":[],"source":["hist = model.fit(X_train, Y_train,\n","           batch_size=batch_size,\n","           epochs=100, verbose=0)"]},{"cell_type":"markdown","metadata":{"id":"jmXIYmxb5zKh"},"source":["Note that I had verbosity turned off to save on some printing space, but feel free to set verbose=1 to see the actual progress of training.\n","\n","Next, let's predict based on some test data and see what our outputs look like"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1710855653294,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"jZgu_T1n5zKh","outputId":"100f8106-d8a3-48ec-f93e-4d4294d26798"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 181ms/step\n","[[ 4. 18. 16.  5. 16. 17. 13.  4. 17.  7.  9. 17. 17. 13. 19.  8.  4.  0.\n","   6.  8.]]\n","[[2.4406708e-04 1.8382258e-05 2.9999010e-05 6.5783228e-05 3.8903388e-05\n","  3.6048408e-05 4.0281215e-05 9.3944291e-05 3.9485414e-05 5.4756376e-05\n","  5.0010152e-05 3.6650163e-05 3.5826914e-05 4.0282059e-05 3.4810513e-05\n","  5.0123886e-05 9.7296994e-05 1.6356021e-03 1.5494174e-04 6.9037451e-05]]\n"]}],"source":["res = model.predict(X_test,batch_size=batch_size)\n","print(X_test[0].T)\n","print(res[0].T)"]},{"cell_type":"markdown","metadata":{"id":"uQbNBLgx5zKi"},"source":["See that in the input data that when there is a -1 that the output switches from values less than 0.5 to values greater than 0.5. In the ideal case this should be a clean separation between 0s and 1s, but typically we need a more powerful model to pull that off.\n","\n",""]},{"cell_type":"markdown","metadata":{"id":"AtLN5E2cD_Oj"},"source":["## Aside: Recurrent vs Recursive Networks  \n","A Recurrent Neural Network is one in which the hidden state of the network is reapplied as a parameter into the network when the next input comes along. The short hand RNN is often used for Recurrent Neural Networks. A Recursive Neural Network meanwhile is a distinct architectural approach in neural networks where we attempt to apply a model in a recursive way over an analysis.\n","\n","This distinction is made clear with an example. Consider the following example of a Recurrent Neural Network. A single network layer produces an output for a given input. While the hidden state is reapplied into the network for the analysis of each individual input token, there is no recursive structure.\n","\n","<!-- recurrent.png\n","Figure sourced without permission from http://www.slideshare.net/jiessiecao/parsing-natural-scenes-and-natural-language-with-recursive-neural-networks  -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1gyvO2IToNQ2J7OXcVLxIyvLi1NnXCxCZ\"/>\n","\n","On the other hand consider the example below of a Recursive Neural Network. Here a model is applied to the total input to produce an output for the total input. That total output is then applied as input to a new higher-order tier of analysis. The output of that analysis can in turn be applied as the input to another higher-order tier of analysis and so forth. Thus the Recursive Neural Network recursively analyses the total input and can be thought of conceptually as being similar to a parser.  \n","\n","<!-- recursive.png\n","Figure sourced without permission from http://www.slideshare.net/jiessiecao/parsing-natural-scenes-and-natural-language-with-recursive-neural-networks\n"," -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1HLhImrJMSUKXHqdgZJYvweBlM6CNrxUv\"/>\n","\n","It should be noted that it is possible to construct Recursive Recurrent Neural Networks. Here a Recurrent Network is used to generate an output for one tier of the analysis, but that output is then fed into the network again to produce a higher order tier. For our purposes here we will stay focused on recurrent neural network architectures."]},{"cell_type":"markdown","metadata":{"id":"RRdWix0QD_OI"},"source":["## Long-Short Term Memory\n","\n","The vanilla RNN that we have looked at to this point is very useful at explaining the basics of how RNNs work, but in practice the vanilla RNN is not strong enough for us to perform complex sequential tasks.\n"]},{"cell_type":"markdown","metadata":{"id":"VsIYZvnTKpIp"},"source":["### The Challenge: Travelling Back in Time!\n","\n","Let's say for example we have a case where a change in the output at T=6 should be dependent on the input at T=1. In an ideal case the backpropogation should allow us to learn this sort of dependency -- but in practice it doesn't. The reason for this is essentially that the learning signal tends to be biased in errors coming from the current time point rather than future time points. We can understand this better by looking at the figure below. The dark coloring indicates the strength of signal between inputs and outputs in either the forward or backward propogation stages. We see that there is a strong relationship between X and Y at the first time step, but because extra layers are present as we move from X at T=1 down to Y at T=2,3,4 etc. that it is more difficult to have a strong signal between X at T=1 and Y's that are further away in time.\n","\n","<!-- bit-1.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1fQINGBbB7r9CpFMDsqnPQse0F2MQnIca\"/>\n","\n","This means that at T=6 for example, we cost function might be able to identify that something should be changed and start passing pack the error derivative to achieve that change, but by the time the error derivative makes it back to the first hidden layer at T=0, that error derivative signal is in practice too small relative to earlier error derivative time signals to be paid attention to.\n","\n","What we need is some way to have more direct control over what error signal passes through the network. Our error signal will still get smaller as it backprrops through time, but we will learn not to corrupt it with error derivatives that just aren't as relevant to us.\n","\n","This is illustrated in the figure below. Circles indicate that the unit is accepting signal for that neuron, while dashes indicates that the unit is closed to input in that direction.\n","\n","<!-- bit-2.png -->\n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1nMYf9IyWn1bDy8QKlqtLmmcEXbGP5TfB\"/>\n","\n","The hand-waving idea here is that there is less leakage or corruption of the error derivative as we move through time. In practice it is a bit more complicated than that -- but not that much.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KxK4v3NnD_Ok"},"source":["### Long Short Term Memory Overview\n","\n","The best known example of an RNN that overcomes this challenge is called the Long Short Term Memory (LSTM) cell. The LSTM cell design is over 20 years old but has proven to be extremely beneficial. It would no longer be considered state-of-the-art for sequence processing tasks -- particularly if you have a lot of computing power to spare -- but for many common sequence processing tasks it is still extensively used.\n","\n","There are many resources available online that provide excellent introductions to the model. The blog post by Christopher Olah is particularly excellent and well known. Rather than repeating that material here, please review Christopher's blog post on LSTM in detail.\n","\n","http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","In the lecture I step through the model, but I refer you to this blog post as a source of information on the model.\n","\n","### Putting LSTM to work: Classification\n","\n","Despite the fact that the LSTM model is considerably more complex than the basic RNN cell, the actual usage of the model within Keras / Tensorflow is practically just as easy as the basic RNN case. Let's see how our text calssification task from the last session can be coverted over to use RNNs first, but then LSTMs instead.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1710855653294,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"M30DUgoCK13S"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, SimpleRNN\n","from tensorflow.keras.datasets import imdb\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"CBwO6_Zux_-K"},"source":["Let's set up some paraemters and load up the dataset."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4934,"status":"ok","timestamp":1710855658223,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"AtrSQ69HyAWZ","outputId":"e6b3d433-3aba-406d-c0b0-829ef6c0d3b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","Train shape: (25000, 400)\n","Test shape: (25000, 400)\n"]}],"source":["max_features = 5000\n","maxlen = 400\n","embedding_dims = 16\n","epochs = 5\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('Train shape:', x_train.shape)\n","print('Test shape:', x_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"08xsz-EkyHxd"},"source":["Let's define the same basic model we used the last time, and then set up the training on it."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1710855658530,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"EG5C1F7PyIDo","outputId":"1b08b1c9-dd08-4803-a74b-96e1c45338b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 400, 16)           80000     \n","                                                                 \n"," flatten (Flatten)           (None, 6400)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 12802     \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 92805 (362.52 KB)\n","Trainable params: 92805 (362.52 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","\n","model.add(Embedding(max_features,\n","                    embedding_dims,\n","                    input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(2, activation='tanh'))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119511,"status":"ok","timestamp":1710855778036,"user":{"displayName":"Robert Ross","userId":"11121896988139609834"},"user_tz":0},"id":"CtpTne4UzWXO","outputId":"2893d47e-b60c-40f7-8c86-41e97ac7b8fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","782/782 [==============================] - 67s 84ms/step - loss: 0.4582 - accuracy: 0.7788 - val_loss: 0.3021 - val_accuracy: 0.8801\n","Epoch 2/5\n","782/782 [==============================] - 19s 25ms/step - loss: 0.2309 - accuracy: 0.9167 - val_loss: 0.2946 - val_accuracy: 0.8790\n","Epoch 3/5\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1380 - accuracy: 0.9583 - val_loss: 0.3137 - val_accuracy: 0.8778\n","Epoch 4/5\n","782/782 [==============================] - 12s 15ms/step - loss: 0.0734 - accuracy: 0.9842 - val_loss: 0.3607 - val_accuracy: 0.8690\n","Epoch 5/5\n","782/782 [==============================] - 10s 13ms/step - loss: 0.0392 - accuracy: 0.9940 - val_loss: 0.4032 - val_accuracy: 0.8658\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7efe02d34310>"]},"metadata":{},"execution_count":14}],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"wiuzWMnmz-Kq"},"source":["Now, let's set up a model that uses the basic RNN."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sgOEoG1l0EtT","outputId":"e646180c-0f5b-4016-c4a6-fd220f69f0d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, None, 16)          80000     \n","                                                                 \n"," simple_rnn_1 (SimpleRNN)    (None, 16)                528       \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 34        \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 80565 (314.71 KB)\n","Trainable params: 80565 (314.71 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/5\n","782/782 [==============================] - 371s 472ms/step - loss: 0.6404 - accuracy: 0.6154 - val_loss: 0.5863 - val_accuracy: 0.6877\n","Epoch 2/5\n","782/782 [==============================] - 358s 459ms/step - loss: 0.4264 - accuracy: 0.8151 - val_loss: 0.4272 - val_accuracy: 0.8201\n","Epoch 3/5\n","782/782 [==============================] - 327s 418ms/step - loss: 0.3170 - accuracy: 0.8768 - val_loss: 0.4120 - val_accuracy: 0.8274\n","Epoch 4/5\n","782/782 [==============================] - 312s 399ms/step - loss: 0.2365 - accuracy: 0.9154 - val_loss: 0.3983 - val_accuracy: 0.8450\n","Epoch 5/5\n"," 31/782 [>.............................] - ETA: 4:35 - loss: 0.2352 - accuracy: 0.9103"]}],"source":["model = Sequential()\n","model.add(Embedding(max_features, embedding_dims))\n","model.add(SimpleRNN(embedding_dims))\n","model.add(Dense(2,activation=\"tanh\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7gx4hbK95zKe"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(max_features, embedding_dims))\n","model.add(LSTM(embedding_dims))\n","model.add(Dense(2,activation=\"tanh\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"yvKkJZzr5zKt"},"source":["Training is now complete - and in principle we could directly use the model now to pass through a complete batch of input data and get the outputs. In practice though it can be handy to reload the model assume a batch input size of 1 only. This makes it easier for us to run one example at a time through the network.\n","\n","Therefore we first recreate our model -- but this time fixing the input layer batch size to one:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sd89gm8v5zKt"},"outputs":[],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(len(vocab), embedding_dim,\n","                          batch_input_shape=[1, None]))\n","model.add(layers.SimpleRNN(rnn_units,return_sequences=True))\n","model.add(tf.keras.layers.Dense(len(vocab)))"]},{"cell_type":"markdown","metadata":{"id":"cOBdrkwZLOWw"},"source":["## Suggested Tasks\n","\n"," 1. Extend the example above to use a combination of filters of length 2, 3 and 4 as suggested on the source article http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n"," 2. Use the pre-trained embeddings layers in place of an on-the-fly embeddings layer"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["A5y9xtQxD_Ob","AtLN5E2cD_Oj","VsIYZvnTKpIp"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":0}